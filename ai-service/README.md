# AI Resume Screener - AI Service

## Overview
Python FastAPI service with **Hybrid Scoring System** combining rule-based (free, fast) and LLM (accurate) approaches for optimal resume matching.

## üöÄ Quick Start
```bash
# 1. Copy environment file
cp .env.example .env

# 2. Set scoring mode (start with free rule-based)
echo "SCORING_MODE=rule_based" >> .env

# 3. Install and run
pip install -r requirements.txt
python -m spacy download en_core_web_sm
python -m uvicorn app.main:app --reload --port 8000
```

See **[QUICK_START_HYBRID.md](QUICK_START_HYBRID.md)** for detailed setup and testing.

## üéØ Hybrid Scoring System

Three modes to choose from:

| Mode | Speed | Cost | Accuracy | Best For |
|------|-------|------|----------|----------|
| **Rule-Based** | ‚ö° Fastest | üí∞ Free | 88% | High volume, MVP |
| **Hybrid** ‚≠ê | ‚ö° Fast | üí∞ Low | 92% | Production (recommended) |
| **LLM Only** | ‚è±Ô∏è Slow | üí∞ High | 90% | Executive search |

### Cost Comparison (1000 candidates)
- Rule-Based: **$0**
- Hybrid: **$0.75-1.50** (90% free, 10% LLM)
- LLM Only: **$7.50-15.00**

See **[HYBRID_SCORING.md](HYBRID_SCORING.md)** for complete documentation.

## Tech Stack
- **Framework**: FastAPI
- **NLP**: Sentence Transformers, spaCy
- **LLM**: Gemini API / OpenAI
- **Vector DB**: FAISS / ChromaDB
- **PDF Parsing**: pdfplumber, PyMuPDF
- **DOCX Parsing**: python-docx

## Architecture Role
This is the **AI/ML Service Layer**:
- Isolated AI operations (cost control)
- Resume text extraction & parsing
- Embedding generation
- Semantic similarity search
- AI-powered scoring
- Interview question generation
- Skill extraction

## Project Structure
```
ai-service/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ api/          # FastAPI routers/endpoints
‚îÇ   ‚îú‚îÄ‚îÄ services/     # Core AI logic
‚îÇ   ‚îú‚îÄ‚îÄ models/       # Pydantic models
‚îÇ   ‚îú‚îÄ‚îÄ utils/        # Helper functions
‚îÇ   ‚îú‚îÄ‚îÄ config.py     # Configuration
‚îÇ   ‚îî‚îÄ‚îÄ main.py       # FastAPI application
‚îú‚îÄ‚îÄ temp/             # Temporary file storage
‚îú‚îÄ‚îÄ vector_store/     # FAISS/Chroma data
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

## Setup Instructions

### 1. Create Virtual Environment
```bash
python -m venv venv

# Windows
venv\Scripts\activate

# Linux/Mac
source venv/bin/activate
```

### 2. Install Dependencies
```bash
pip install -r requirements.txt

# Download spaCy model
python -m spacy download en_core_web_sm
```

### 3. Configure Environment
```bash
cp .env.example .env
# Add your API keys (Gemini/OpenAI)
```

### 4. Run Development Server
```bash
uvicorn app.main:app --reload --port 8000
```

### 5. View API Documentation
- Swagger UI: http://localhost:8000/docs
- ReDoc: http://localhost:8000/redoc

## API Endpoints

### Resume Parsing
- `POST /api/parse/pdf` - Extract text from PDF resume
- `POST /api/parse/docx` - Extract text from DOCX resume
- `POST /api/parse/extract-skills` - Extract skills from text
- `POST /api/parse/extract-experience` - Extract experience details

### Embeddings
- `POST /api/embeddings/generate` - Generate embedding for text
- `POST /api/embeddings/batch` - Generate embeddings for multiple texts
- `GET /api/embeddings/models` - List available embedding models

### Semantic Search
- `POST /api/search/similarity` - Find similar resumes to job
- `POST /api/search/rank-candidates` - Rank candidates by match
- `GET /api/search/vector-stats` - Get vector store statistics

### Scoring
- `POST /api/score/match` - Calculate match score
- `POST /api/score/explain` - Generate match explanation
- `POST /api/score/skill-overlap` - Analyze skill overlap

### Interview Generation
- `POST /api/interview/generate` - Generate interview kit
- `POST /api/interview/technical` - Generate technical questions
- `POST /api/interview/behavioral` - Generate behavioral questions

### Health & Monitoring
- `GET /health` - Health check
- `GET /metrics` - Service metrics

## AI Models Used

### Embedding Model
- **Default**: sentence-transformers/all-MiniLM-L6-v2
- **Size**: 80MB
- **Dimensions**: 384
- **Speed**: Fast
- **Quality**: Good for semantic search

### LLM Options
1. **Gemini 1.5 Pro** (Recommended for free tier)
   - Free quota: Generous
   - Quality: Excellent
   - Cost: $0/month for moderate usage

2. **OpenAI GPT-4 Turbo**
   - Quality: Premium
   - Cost: Pay-per-token

## Environment Variables
See `.env.example` for all configuration options.

## Caching Strategy
- Embeddings cached (reuse for same text)
- LLM responses cached (reduce cost)
- Vector indexes persisted to disk

## Performance Optimization
- Batch embedding generation
- Async file processing
- Vector index caching
- Rate limiting on LLM calls

## Deployment
- **Phase-1**: Render/Railway
- **Phase-3**: AWS ECS with GPU (optional)

## Cost Control
- LLM calls tracked per tenant
- Rate limiting enforced
- Caching reduces API calls
- Free tier prioritized

## License
MIT
